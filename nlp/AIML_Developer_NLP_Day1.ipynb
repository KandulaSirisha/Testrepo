{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f92f3e",
   "metadata": {},
   "source": [
    "1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?\n",
    "2. Describe tokenization in NLP and explain its significance in text processing.\n",
    "3. What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?\n",
    "4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?\n",
    "5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?\n",
    "6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?\n",
    "7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?\n",
    "8. Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cd13e",
   "metadata": {},
   "source": [
    "# 1.What is the purpose of text preprocessing in NLP, and why is it essential before analysis?\n",
    "Text preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming raw text data into a format suitable for analysis. There are several reasons why text preprocessing is essential:\n",
    "\n",
    "-Noise Reduction: Text data often contains noise, such as special characters, punctuation, HTML tags, or irrelevant symbols. Preprocessing helps eliminate these elements to focus on meaningful content.\n",
    "\n",
    "-Normalization: It standardizes text by converting everything to lowercase, which ensures uniformity and prevents the model from treating words with different cases as different entities. Additionally, it involves stemming or lemmatization to reduce words to their root form, aiding in capturing the essence of the word without considering its various forms.\n",
    "\n",
    "-Tokenization: Breaking text into smaller units, like words or sentences (tokens), helps in further analysis. Tokenization simplifies the text by segmenting it into manageable parts for analysis.\n",
    "\n",
    "-Removing Stopwords: Words like \"and,\" \"the,\" or \"is\" don't usually contribute significant meaning to the text and can be removed to reduce noise and improve computational efficiency.\n",
    "\n",
    "-Vectorization: Converting text into numerical representations (word embeddings or vectors) is essential for machine learning models. This step makes the text data understandable by the algorithms, allowing them to process and analyze it effectively.\n",
    "\n",
    "-Feature Engineering: Text preprocessing helps in feature creation or extraction, enabling the model to learn more efficiently from the data. It includes techniques like n-gram generation or creating features based on domain-specific knowledge.\n",
    "\n",
    "Text preprocessing ensures that the NLP model can focus on the meaningful aspects of the text data, removing irrelevant elements that might hinder accurate analysis. It helps in building robust models that can generalize well to new, unseen data and improve the overall performance of NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a52ecf",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing.\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens, which can be words, phrases, symbols, or other meaningful elements. In natural language processing (NLP), tokenization plays a crucial role in text processing for several reasons:\n",
    "\n",
    "1. Text Segmentation: Tokenization breaks down raw text into smaller units, making it more manageable for analysis. These tokens can be individual words (word-level tokenization), characters (character-level tokenization), or even phrases (phrase-level tokenization).\n",
    "\n",
    "2. Preparation for Analysis: By breaking text into tokens, NLP models can process and understand language more effectively. Tokens serve as the basic units for various NLP tasks, including sentiment analysis, machine translation, named entity recognition, and part-of-speech tagging, among others.\n",
    "\n",
    "3. Vocabulary Creation: Tokenization helps create a vocabulary or a dictionary of unique tokens in a corpus. Each token typically represents a distinct feature in the text data, forming the basis for further analysis and model training.\n",
    "\n",
    "4. Normalization and Standardization: Tokenization assists in normalizing and standardizing text data. For instance, converting all words to lowercase or handling punctuation marks consistently can aid in ensuring uniformity in the dataset.\n",
    "\n",
    "5. Handling Ambiguity: In languages where words can have multiple meanings (homographs), tokenization helps disambiguate by separating such tokens based on context or parts of speech, aiding in more accurate analysis.\n",
    "\n",
    "6. Feature Extraction: Tokens serve as the foundation for feature extraction in NLP. Techniques such as n-grams or bag-of-words models rely on tokenization to create features that capture the essence of the text for machine learning algorithms.\n",
    "\n",
    "7. Removing Stopwords: Tokenization allows for the identification and removal of stopwords—common words like \"and,\" \"the,\" or \"is\"—which don't contribute significantly to the meaning of the text and can be excluded from analysis.\n",
    "\n",
    "Overall, tokenization is a fundamental step in NLP that breaks text into meaningful units, enabling machines to understand and process human language effectively. It acts as a foundational step for various higher-level NLP tasks and facilitates the extraction of valuable information from textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee24de",
   "metadata": {},
   "source": [
    "# 3.What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?\n",
    "Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce words to their root forms, but they have differences in their approaches and outputs:\n",
    "\n",
    "Stemming:\n",
    "- Definition: Stemming involves cutting off prefixes or suffixes of words to extract the root or base form, known as the stem. It's a rule-based and heuristic approach that doesn't always result in actual words.\n",
    "- Example: For the word \"running,\" the stem would be \"run.\" Stemming might produce \"runn\" for \"running\" or \"happi\" for \"happiness.\"\n",
    "- Speed: Stemming is typically faster than lemmatization because it applies simple rules without considering the context of the word.\n",
    "- Use Cases: Stemming is often used in applications where speed is crucial, such as information retrieval or indexing. It's less concerned with linguistic accuracy and more focused on reducing words to a common base form for simplicity and speed.\n",
    "\n",
    "Lemmatization:\n",
    "- Definition: Lemmatization, on the other hand, uses vocabulary analysis and morphological analysis to return the base or dictionary form of a word, known as the lemma. It employs linguistic rules and context to ensure that the resulting lemma is an actual word.\n",
    "- Example: For the word \"better,\" the lemma remains \"better\" as it is the base form, not a modification. Lemmatization aims for accuracy by considering the word's part of speech and its meaning in context.\n",
    "- Accuracy: Lemmatization tends to be more accurate than stemming because it considers the context and linguistic rules of the language, which means it might be slower.\n",
    "- Use Cases: Lemmatization is suitable for tasks where accuracy and precision are essential, like language understanding, machine translation, sentiment analysis, or any application that relies on understanding the meaning of words in context.\n",
    "\n",
    "**Choosing Between Stemming and Lemmatization:**\n",
    "- Use stemming if speed is critical and you can compromise some linguistic accuracy.\n",
    "- Choose lemmatization when accuracy and precision in understanding the language are crucial, even if it means sacrificing speed.\n",
    "- Consider the specific requirements of your NLP task. For information retrieval or search engines where speed matters, stemming might be more appropriate. For applications focused on language understanding or sentiment analysis, lemmatization might yield better results despite its slower processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f589a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SIRISHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown foxes jumped over the lazy dogs\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Initialize Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0af02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SIRISHA\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown foxes jumped over the lazy dogs\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizing each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08469ae4",
   "metadata": {},
   "source": [
    "# 4.Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?\n",
    "Stop words are common words in a language that are often filtered out during text preprocessing in natural language processing (NLP). These words, such as \"the,\" \"and,\" \"is,\" \"in,\" etc., occur frequently in a corpus of text but typically carry less meaningful information compared to content-bearing words like nouns, verbs, or adjectives.\n",
    "\n",
    "Role in Text Preprocessing:\n",
    "1. Noise Reduction: Stop words are considered noise in text data. Removing them helps reduce the amount of noise in the text, allowing NLP algorithms to focus more on the important content words.\n",
    "\n",
    "2. Efficiency: Filtering out stop words can improve the efficiency of text processing algorithms, as it reduces the amount of data to be processed. This can speed up tasks like indexing, search, or information retrieval.\n",
    "\n",
    "3. Normalization: Removing stop words can aid in standardizing the text, making it more consistent and easier to analyze.\n",
    "\n",
    "Impact on NLP Tasks:\n",
    "1. Document Retrieval: In tasks like information retrieval or search engines, removing stop words can improve the accuracy and relevance of search results by focusing on content-bearing words.\n",
    "\n",
    "2. Topic Modeling: Stop words can interfere with topic modeling algorithms such as Latent Dirichlet Allocation (LDA). Removing them helps in identifying meaningful topics by focusing on important terms.\n",
    "\n",
    "3. Sentiment Analysis: Stop words might not contribute much to sentiment or emotion in text. Removing them can sometimes improve the accuracy of sentiment analysis models by focusing on sentiment-bearing words.\n",
    "\n",
    "4. Language Models: Stop words might not be essential in language modeling tasks such as machine translation, where preserving content-bearing words is crucial for accurate translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9cbd3b",
   "metadata": {},
   "source": [
    "# Sentence Tokenization - spliting as a sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782dd8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "====================================================================================================\n",
      "Hello Mr.John,Hope you are doing good.\n",
      "By the way I have a plan to visit to your house in the next week of the month\n",
      "====================================================================================================\n",
      "\n",
      "After Sentence Tokenization:\n",
      " ['Hello Mr.John,Hope you are doing good.', 'By the way I have a plan to visit to your house in the next week of the month']\n",
      "====================================================================================================\n",
      "No.of Sentences:\t 2\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"\"\"Hello Mr.John,Hope you are doing good.\n",
    "By the way I have a plan to visit to your house in the next week of the month\"\"\"\n",
    "print('Original text:')\n",
    "print('='*100)\n",
    "print(text)\n",
    "print('='*100)\n",
    "print()\n",
    "tokenised_sent=sent_tokenize(text)\n",
    "print('After Sentence Tokenization:\\n',tokenised_sent)\n",
    "print('='*100)\n",
    "print('No.of Sentences:\\t',len(tokenised_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02ff39",
   "metadata": {},
   "source": [
    "# Word Tokenization - Spliting as a words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02344fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "====================================================================================================\n",
      "Hello Mr.John,Hope you are doing good.\n",
      "By the way I have a plan to visit to your house in the next week of the month\n",
      "====================================================================================================\n",
      "\n",
      "After word Tokenization:\n",
      " ['Hello', 'Mr.John', ',', 'Hope', 'you', 'are', 'doing', 'good', '.', 'By', 'the', 'way', 'I', 'have', 'a', 'plan', 'to', 'visit', 'to', 'your', 'house', 'in', 'the', 'next', 'week', 'of', 'the', 'month']\n",
      "====================================================================================================\n",
      "No.of words:\t 28\n",
      "====================================================================================================\n",
      "word :  Hello & Length : 5\n",
      "word :  Mr.John & Length : 7\n",
      "word :  , & Length : 1\n",
      "word :  Hope & Length : 4\n",
      "word :  you & Length : 3\n",
      "word :  are & Length : 3\n",
      "word :  doing & Length : 5\n",
      "word :  good & Length : 4\n",
      "word :  . & Length : 1\n",
      "word :  By & Length : 2\n",
      "word :  the & Length : 3\n",
      "word :  way & Length : 3\n",
      "word :  I & Length : 1\n",
      "word :  have & Length : 4\n",
      "word :  a & Length : 1\n",
      "word :  plan & Length : 4\n",
      "word :  to & Length : 2\n",
      "word :  visit & Length : 5\n",
      "word :  to & Length : 2\n",
      "word :  your & Length : 4\n",
      "word :  house & Length : 5\n",
      "word :  in & Length : 2\n",
      "word :  the & Length : 3\n",
      "word :  next & Length : 4\n",
      "word :  week & Length : 4\n",
      "word :  of & Length : 2\n",
      "word :  the & Length : 3\n",
      "word :  month & Length : 5\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print('Original text:')\n",
    "print('='*100)\n",
    "print(text)\n",
    "print('='*100)\n",
    "print()\n",
    "token_word=word_tokenize(text)\n",
    "print('After word Tokenization:\\n',token_word)\n",
    "print('='*100)\n",
    "print('No.of words:\\t',len(token_word))\n",
    "print('='*100)\n",
    "for i in token_word:\n",
    "    print('word : ',i,'&','Length :',len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6990599",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2064edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.FreqDist'>\n",
      "<FreqDist with 25 samples and 28 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq_dist=FreqDist(token_word)\n",
    "print(type(freq_dist))\n",
    "print(freq_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbdfda6",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1050ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No.of Stopwords:\t 179\n",
      "====================================================================================================\n",
      "The Stop words are:\n",
      "\n",
      "{'once', 'most', \"don't\", 'needn', 'there', \"isn't\", 'do', 'hers', 'only', \"it's\", \"mightn't\", 'out', 'for', 'mustn', 'what', 'isn', 'between', 'no', 'because', 'i', 'has', 'hadn', 'ma', 'some', 'before', 'myself', \"that'll\", 'doing', 'off', 'each', 'why', 'ourselves', 'y', 'other', 'who', 'are', 'against', 'during', 'yourself', \"couldn't\", \"wouldn't\", 'will', 'doesn', \"haven't\", 'weren', \"you're\", 'ain', 'so', 'those', 'below', 'after', 'or', 'through', 'll', 'couldn', 'won', 'into', 'a', 'mightn', 'not', 'whom', 'am', 'be', 'up', 'further', 'very', 't', 're', 'were', 'by', 'shouldn', 'while', 'then', 'that', 'd', 'about', 'where', 'having', \"mustn't\", 'themselves', 'from', 'ours', \"didn't\", 'we', \"weren't\", 'which', 'had', 'all', 'same', 'their', 'it', 'with', 'should', 'here', 'didn', 'our', 'me', \"you'd\", 'these', 'your', 'they', 'them', 'shan', \"won't\", 'under', 'nor', 'himself', 'if', 'been', 'o', 'more', 'don', 'just', 'and', 'yourselves', 'above', 'itself', 'its', 'hasn', 'he', \"aren't\", 'herself', 'have', 'over', 'any', 'down', 'as', 'she', \"shan't\", 'was', 'his', 'did', 'now', 'is', \"should've\", 've', 'at', 'can', 'own', 'being', \"you'll\", 'theirs', \"you've\", 'until', 'my', 'again', 'too', 'an', \"doesn't\", 'wasn', 'the', 'haven', 'how', 'him', 'yours', 'both', 'when', 'this', \"she's\", \"hasn't\", 'on', 's', \"needn't\", \"hadn't\", 'you', 'few', 'than', 'but', 'aren', \"wasn't\", 'wouldn', 'in', 'such', \"shouldn't\", 'm', 'of', 'to', 'does', 'her'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SIRISHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "print('Total No.of Stopwords:\\t',len(stop_words))\n",
    "print('='*100)\n",
    "print('The Stop words are:\\n')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f609a7",
   "metadata": {},
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa6bd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words:\t 28\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tokenized words-with stopwords:\n",
      "\n",
      "\t ['Hello', 'Mr.John', ',', 'Hope', 'you', 'are', 'doing', 'good', '.', 'By', 'the', 'way', 'I', 'have', 'a', 'plan', 'to', 'visit', 'to', 'your', 'house', 'in', 'the', 'next', 'week', 'of', 'the', 'month']\n",
      "====================================================================================================\n",
      "Length after Remove stopwords:\t 15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      " After Removing stopwords- words are:\n",
      "\t ['Hello', 'Mr.John', ',', 'Hope', 'good', '.', 'By', 'way', 'I', 'plan', 'visit', 'house', 'next', 'week', 'month']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens=[]\n",
    "for w in token_word:\n",
    "    if w not in stop_words:\n",
    "         filtered_tokens.append(w)\n",
    "print('Length of words:\\t',len(token_word))\n",
    "print('-'*100)\n",
    "print('Tokenized words-with stopwords:\\n\\n\\t',token_word)\n",
    "print('='*100)\n",
    "print('Length after Remove stopwords:\\t',len(filtered_tokens))\n",
    "print('-'*100)\n",
    "print('\\n After Removing stopwords- words are:\\n\\t',filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89af2c3",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b68e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SIRISHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2061bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentences:\n",
      "\n",
      "\thow was your day?\n",
      "====================================================================================================\n",
      "Tokens of word:\n",
      " ['how', 'was', 'your', 'day', '?']\n",
      "Length: \t 5\n",
      "****************************************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original text:\n",
      " how was your day?\n",
      "====================================================================================================\n",
      "========================================\n",
      "POS Tagging ('how', 'WRB')\n",
      "========================================\n",
      "========================================\n",
      "POS Tagging ('was', 'VBD')\n",
      "========================================\n",
      "========================================\n",
      "POS Tagging ('your', 'PRP$')\n",
      "========================================\n",
      "========================================\n",
      "POS Tagging ('day', 'NN')\n",
      "========================================\n",
      "========================================\n",
      "POS Tagging ('?', '.')\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "#prompt to take input string\n",
    "text=input('Enter a sentences:\\n\\n\\t')\n",
    "#split as word tokens\n",
    "words=word_tokenize(text)\n",
    "print('='*100)\n",
    "print('Tokens of word:\\n',words)\n",
    "print('Length: \\t',len(words))\n",
    "print('*'*100)\n",
    "#Apply POS tag to each word\n",
    "pos_tags=pos_tag(words)\n",
    "print('-'*100)\n",
    "print('Original text:\\n',text)\n",
    "print('='*100)\n",
    "for w in pos_tags:\n",
    "    print('='*40)\n",
    "    print('POS Tagging',w)\n",
    "    print('='*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e40df",
   "metadata": {},
   "source": [
    "# Filter out Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1656bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentences:\n",
      "\n",
      "\tHello Mr.John,Hope you are doing good.\n",
      "====================================================================================================\n",
      "Length: \t 9\n",
      "Tokens of word:\n",
      " ['Hello', 'Mr.John', ',', 'Hope', 'you', 'are', 'doing', 'good', '.']\n",
      "****************************************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original text:\n",
      " Hello Mr.John,Hope you are doing good.\n",
      "====================================================================================================\n",
      "After Removing Punctuation:\n",
      "\n",
      "['Hello', 'Hope', 'you', 'are', 'doing', 'good'] 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "#prompt to take input string\n",
    "text=input('Enter a sentences:\\n\\n\\t')\n",
    "#split as word tokens\n",
    "tokens=word_tokenize(text)\n",
    "print('='*100)\n",
    "print('Length: \\t',len(tokens))\n",
    "print('Tokens of word:\\n',tokens)\n",
    "print('*'*100)\n",
    "#Remove all tokens that are not alphabetic\n",
    "words=[word for word in tokens if word.isalpha()]\n",
    "print('-'*100)\n",
    "print('Original text:\\n',text)\n",
    "print('='*100)\n",
    "print('After Removing Punctuation:\\n')\n",
    "print(words,len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001bc85",
   "metadata": {},
   "source": [
    "# 6.Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks? \n",
    "Lowercasing text in NLP is crucial for several reasons:\n",
    "\n",
    "1. Uniformity:It ensures consistent representation of words regardless of their casing, preventing the model from treating words with different cases as distinct entities.\n",
    "\n",
    "2. Normalization:It reduces the vocabulary size by merging words that differ only in their capitalization, improving the efficiency of language processing tasks.\n",
    "\n",
    "3. Matching and Comparison: Lowercasing enables effective matching and comparison between words, making it easier to identify similarities and perform text analysis accurately.\n",
    "\n",
    "4. Preventing Redundancy: Lowercasing helps in avoiding duplicate entries in the vocabulary due to variations in capitalization, leading to more effective model training and better generalization.\n",
    "\n",
    "Overall, lowercase conversion in text preprocessing is a standard step in NLP tasks that promotes consistency, reduces redundancy, and improves the efficiency and accuracy of language processing models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa69137",
   "metadata": {},
   "source": [
    "# 7.Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?\n",
    "\n",
    "Vectorization in the context of text data refers to the process of converting text into numerical vectors that machine learning algorithms can understand and process. Techniques like CountVectorizer are instrumental in this process within natural language processing (NLP).\n",
    "\n",
    "CountVectorizer:\n",
    "- CountVectorizer is a technique used to convert a collection of text documents into a matrix of token counts. Each document is represented by a vector, where each element corresponds to the count of a particular word (token) in the document.\n",
    "- It creates a vocabulary of unique words from the entire corpus and assigns an index to each word. Then, for each document, it counts the occurrences of each word from the vocabulary and forms a vector representation of the document.\n",
    "- Stop words, punctuation, and lowercase conversion are often used in conjunction with CountVectorizer to preprocess the text and create meaningful vectors without noise or unnecessary variations.\n",
    "\n",
    "Contribution to Text Preprocessing in NLP:\n",
    "- Numerical Representation: Vectorization transforms raw text data into numerical form, allowing machine learning algorithms to process and analyze the text effectively.\n",
    "- Feature Creation:It creates a structured representation of text that serves as features for machine learning models. These features capture the frequency of words in a document or corpus, enabling the model to learn patterns and relationships.\n",
    "- Input to Models: The numerical vectors generated by CountVectorizer or similar techniques serve as input to various NLP models like classification algorithms, clustering methods, or recommendation systems.\n",
    "- Sparse Matrix: The output of CountVectorizer is often a sparse matrix, efficiently representing the text data while conserving memory by storing only the non-zero elements.\n",
    "\n",
    "By converting text into numerical vectors, techniques like CountVectorizer contribute significantly to text preprocessing in NLP, enabling machines to interpret and analyze textual information, and facilitating the application of machine learning algorithms to solve various language-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20482e",
   "metadata": {},
   "source": [
    "# Word counts with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b816ea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text paragraph please:\n",
      "\n",
      "\tThis is the first document,This document is the second document, And this is the third one.Lets see the vector form.\n",
      "Sentences as tokens:\n",
      "This is the first document,This document is the second document, And this is the third one.Lets see the vector form.\n",
      "Feature names:\n",
      " ['and' 'document' 'first' 'form' 'is' 'lets' 'one' 'second' 'see' 'the'\n",
      " 'third' 'this' 'vector']\n",
      "====================================================================================================\n",
      "Token Counts Matrix:\n",
      "[[1 3 1 1 3 1 1 1 1 4 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents=input('Text paragraph please:\\n\\n\\t')\n",
    "sent_tokens=sent_tokenize(documents)\n",
    "print('Sentences as tokens:')\n",
    "for sent in sent_tokens:\n",
    "    print(sent)\n",
    "\n",
    "#Initialize countvectorizer\n",
    "vector=CountVectorizer()\n",
    "#Fit and transform the documents\n",
    "x=vector.fit_transform(sent_tokens)\n",
    "#get the feature names (words)\n",
    "feature_names=vector.get_feature_names_out()\n",
    "#Display the matrix of token counts\n",
    "print('Feature names:\\n',feature_names)\n",
    "print('='*100)\n",
    "print('Token Counts Matrix:')\n",
    "print(x.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1196759",
   "metadata": {},
   "source": [
    "# 8.Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing.\n",
    "\n",
    "Normalization in NLP refers to the process of transforming text data into a more standardized or normalized form. It aims to make the text consistent, reducing variations and ensuring uniform representation across the dataset. Normalization helps in improving the accuracy of text analysis and the performance of machine learning models by treating similar words or forms identically.\n",
    "\n",
    "Some common normalization techniques used in text preprocessing include:\n",
    "\n",
    "1. Lowercasing: Converting all text to lowercase ensures uniformity in the dataset. For instance, converting \"Hello,\" \"hello,\" and \"HELLO\" to \"hello\" treats them as the same word.\n",
    "\n",
    "2. Stemming: Stemming reduces words to their base or root form by removing prefixes or suffixes. For example, reducing \"running\" and \"runs\" to the stem \"run.\"\n",
    "\n",
    "3. Lemmatization: Lemmatization also reduces words to their base form but uses linguistic rules and context to ensure the resulting word is a valid one. For instance, \"better\" remains \"better\" as its lemma.\n",
    "\n",
    "4. Removing Accents or Diacritics: Normalizing by removing accents or diacritical marks from text ensures consistency in words that might have variations due to accents, such as café and cafe.\n",
    "\n",
    "5. Removing Special Characters and Symbols: Eliminating non-alphanumeric characters, punctuation, or symbols from text helps in reducing noise and ensuring better processing of text data.\n",
    "\n",
    "6. Handling Contractions and Abbreviations: Expanding contractions (e.g., converting \"don't\" to \"do not\") or normalizing abbreviations to their full forms helps maintain consistency.\n",
    "\n",
    "7. Numeric Normalization: Converting numbers into a standard format (e.g., replacing digits with a generic token like \"      <NUM>\")to treat all numerical expressions uniformly.\n",
    "\n",
    "Normalization techniques play a crucial role in preparing text data for analysis in NLP tasks. They assist in standardizing the text, reducing vocabulary size, and ensuring that the machine learning models can effectively learn and generalize from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "095b6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs']\n",
      "Stemmed: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dogs\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Lowercasing\n",
    "lowercased_words = [word.lower() for word in words]\n",
    "print(\"Lowercased:\", lowercased_words)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in lowercased_words]\n",
    "print(\"Stemmed:\", stemmed_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
