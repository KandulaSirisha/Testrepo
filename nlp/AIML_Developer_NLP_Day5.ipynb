{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcf28d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dc3f3ff2034d69816bd1ea47dfce63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIRISHA\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SIRISHA\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d4495d41b34a669ab3ec8db62a182a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252a0ccfcd2c4a3c97ddbf9de1f7afc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a1c1899c47429998580f43862d3e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c07df50ec44a41b620224634da8977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What does Hugging Face do?\n",
      "Answer: provides state-of-the-art natural language processing models\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def build_qa_model():\n",
    "    qa_model = pipeline('question-answering', model='distilbert-base-cased-distilled-squad', tokenizer='distilbert-base-cased-distilled-squad')\n",
    "    return qa_model\n",
    "\n",
    "def ask_question(qa_model, context, question):\n",
    "    result = qa_model(context=context, question=question)\n",
    "    return result['answer']\n",
    "\n",
    "context = \"Hugging Face is a company that provides state-of-the-art natural language processing models.\"\n",
    "question = \"What does Hugging Face do?\"\n",
    "\n",
    "qa_model = build_qa_model()\n",
    "answer = ask_question(qa_model, context, question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce50a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting sentencepiece\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/cc/07/d6951e3b4079df819d76353302fc3e76835252e7e0b6366f96a03d87ea5f/sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     - -------------------------------------- 41.0/977.5 kB ? eta -:--:--\n",
      "     --- --------------------------------- 92.2/977.5 kB 871.5 kB/s eta 0:00:02\n",
      "     ------- ------------------------------ 184.3/977.5 kB 1.4 MB/s eta 0:00:01\n",
      "     --------- ---------------------------- 235.5/977.5 kB 1.2 MB/s eta 0:00:01\n",
      "     ---------- --------------------------- 276.5/977.5 kB 1.2 MB/s eta 0:00:01\n",
      "     ------------ ------------------------- 327.7/977.5 kB 1.1 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 481.3/977.5 kB 1.4 MB/s eta 0:00:01\n",
      "     ----------------------- -------------- 593.9/977.5 kB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 655.4/977.5 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 665.6/977.5 kB 1.5 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 747.5/977.5 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 839.7/977.5 kB 1.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 921.6/977.5 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 977.5/977.5 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888d139e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fd2688bb2746899eda3ec330f6c9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIRISHA\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SIRISHA\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98a565f71344842a37744d286228592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783e15965e9046e5abea7e3377c32a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b27becbbaeb4e64b26860263c6d23c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Text: Hello, how are you?\n",
      "Translated Text: Bonjour, comment allez-vous?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def build_translation_model(model_name):\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def translate_text(model, tokenizer, text, source_lang, target_lang):\n",
    "    # Tokenize the input text with source language token\n",
    "    input_ids = tokenizer.encode(f\">>{source_lang}<< {text}\", return_tensors=\"pt\")\n",
    "\n",
    "    # Generate translation without specifying the language\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # Decode the output, skipping special tokens and removing source language token\n",
    "    translation = tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(f\" >>{source_lang}<<\", \"\")\n",
    "    return translation\n",
    "\n",
    "# Example usage:\n",
    "source_lang = \"en\"  # Source language (e.g., English)\n",
    "target_lang = \"fr\"  # Target language (e.g., French)\n",
    "\n",
    "model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n",
    "translation_model, translation_tokenizer = build_translation_model(model_name)\n",
    "\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "translated_text = translate_text(translation_model, translation_tokenizer, text_to_translate, source_lang, target_lang)\n",
    "\n",
    "print(f\"Source Text: {text_to_translate}\")\n",
    "print(f\"Translated Text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6a1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
