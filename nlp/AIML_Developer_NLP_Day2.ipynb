{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee4f246",
   "metadata": {},
   "source": [
    "# You are part of a team developing a text classification system for a news aggregator platform. The platform aims to categorize news articles into different topics automatically. The dataset contains news articles along with their corresponding topics. Perform only the Feature extraction techniques.\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/datasets/therohk/million-headlines\n",
    "\n",
    "Data Exploration: Begin by exploring the dataset. What are the different topics/categories present in the dataset? What is the distribution of articles across these topics?\n",
    "\n",
    "Bag-of-Words (BoW): Implement a Bag-of-Words (BoW) model using Count Vectorizer or TF-IDF to transform the text data into numerical features. Discuss the advantages and limitations of Bow in this context. Apply both unigram and bigram techniques and compare their effects on classification accuracy.\n",
    "\n",
    "N-grams: Explore the use of N-grams (bi-grams, tri-grams) in feature engineering. How do different N-gram ranges impact the performance of the classification model?\n",
    "\n",
    "TF-IDF: Apply TF-IDF (Term Frequency-Inverse Document Frequency) to the text data. Describe how TF-IDF works and its significance in capturing the importance of words across documents. Compare the results of TF-IDF with the BoW approach.\n",
    "\n",
    "One-Hot Encoding: Investigate the application of One-Hot Encoding to encode categorical variables or labels. Can One-Hot Encoding be used directly for text classification? Why or why not?\n",
    "\n",
    "Deliverables:\n",
    "\n",
    "Present insights gathered from data exploration and discuss the impact of different feature engineering techniques (BoW, N-grams, TF-IDF, One-Hot Encoding). Provide recommendations for the best feature engineering strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d028588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Count Vectorizer (Unigrams): 0.36666666666666664\n",
      "Accuracy with Count Vectorizer (Bigrams): 0.4166666666666667\n",
      "Accuracy with TF-IDF Vectorizer (Unigrams): 0.3416666666666667\n",
      "Accuracy with TF-IDF Vectorizer (Bigrams): 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    df['headline_text'], df['publish_date'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Bag-of-Words (BoW) with Count Vectorizer (Unigrams)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(train_data)\n",
    "X_test_count = count_vectorizer.transform(test_data)\n",
    "\n",
    "# Bag-of-Words (BoW) with Count Vectorizer (Bigrams)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X_train_bigram = bigram_vectorizer.fit_transform(train_data)\n",
    "X_test_bigram = bigram_vectorizer.transform(test_data)\n",
    "\n",
    "# Bag-of-Words (BoW) with TF-IDF Vectorizer (Unigrams)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Bag-of-Words (BoW) with TF-IDF Vectorizer (Bigrams)\n",
    "tfidf_bigram_vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "X_train_tfidf_bigram = tfidf_bigram_vectorizer.fit_transform(train_data)\n",
    "X_test_tfidf_bigram = tfidf_bigram_vectorizer.transform(test_data)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier on each representation\n",
    "model_count = MultinomialNB()\n",
    "model_count.fit(X_train_count, train_labels)\n",
    "predictions_count = model_count.predict(X_test_count)\n",
    "accuracy_count = accuracy_score(test_labels, predictions_count)\n",
    "print(f\"Accuracy with Count Vectorizer (Unigrams): {accuracy_count}\")\n",
    "\n",
    "model_bigram = MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, train_labels)\n",
    "predictions_bigram = model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(test_labels, predictions_bigram)\n",
    "print(f\"Accuracy with Count Vectorizer (Bigrams): {accuracy_bigram}\")\n",
    "\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, train_labels)\n",
    "predictions_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(test_labels, predictions_tfidf)\n",
    "print(f\"Accuracy with TF-IDF Vectorizer (Unigrams): {accuracy_tfidf}\")\n",
    "\n",
    "model_tfidf_bigram = MultinomialNB()\n",
    "model_tfidf_bigram.fit(X_train_tfidf_bigram, train_labels)\n",
    "predictions_tfidf_bigram = model_tfidf_bigram.predict(X_test_tfidf_bigram)\n",
    "accuracy_tfidf_bigram = accuracy_score(test_labels, predictions_tfidf_bigram)\n",
    "print(f\"Accuracy with TF-IDF Vectorizer (Bigrams): {accuracy_tfidf_bigram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb432c",
   "metadata": {},
   "source": [
    "# Advantages of Bag-of-Words (BoW):\n",
    "Simple Representation: BoW is a simple and effective way to represent text data numerically.\n",
    "Interpretability: The generated features (word frequencies or TF-IDF scores) are interpretable and can provide insights into the importance of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0a2cd",
   "metadata": {},
   "source": [
    "# Limitations of Bag-of-Words (BoW):\n",
    "Loss of Word Order: BoW discards the order of words in the text, which may result in a loss of important syntactic and semantic information.\n",
    "High Dimensionality: The feature space can become very large, especially with a large vocabulary or when using n-grams, leading to increased computational requirements and potential overfitting.\n",
    "Ignores Context: BoW treats each word independently and doesn't consider the context in which words appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef77d3",
   "metadata": {},
   "source": [
    "# Comparison of Unigrams and Bigrams:\n",
    "Unigrams: Capture individual words and their frequencies. Suitable for capturing the overall vocabulary and word usage patterns.\n",
    "Bigrams: Consider pairs of consecutive words. Useful for capturing some level of context and understanding phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dbe3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Bi-grams: 0.4166666666666667\n",
      "Accuracy with Tri-grams: 0.4083333333333333\n"
     ]
    }
   ],
   "source": [
    "# Explore N-grams (Bi-grams and Tri-grams)\n",
    "# Using Bi-grams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X_train_bigram = bigram_vectorizer.fit_transform(train_data)\n",
    "X_test_bigram = bigram_vectorizer.transform(test_data)\n",
    "\n",
    "# Using Tri-grams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X_train_trigram = trigram_vectorizer.fit_transform(train_data)\n",
    "X_test_trigram = trigram_vectorizer.transform(test_data)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier on each representation\n",
    "model_bigram = MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, train_labels)\n",
    "predictions_bigram = model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(test_labels, predictions_bigram)\n",
    "print(f\"Accuracy with Bi-grams: {accuracy_bigram}\")\n",
    "\n",
    "model_trigram = MultinomialNB()\n",
    "model_trigram.fit(X_train_trigram, train_labels)\n",
    "predictions_trigram = model_trigram.predict(X_test_trigram)\n",
    "accuracy_trigram = accuracy_score(test_labels, predictions_trigram)\n",
    "print(f\"Accuracy with Tri-grams: {accuracy_trigram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5bf6d0",
   "metadata": {},
   "source": [
    "# Impact on Performance:\n",
    "Bi-grams and Tri-grams can improve model performance when the task involves capturing longer-range dependencies in the text.\n",
    "However, higher-order N-grams also increase the dimensionality of the feature space, which might lead to increased computational requirements and potential overfitting.\n",
    "The optimal choice of N-grams depends on the specific characteristics of the text data and the nature of the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8904bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with BoW: 0.36666666666666664\n",
      "Accuracy with TF-IDF: 0.3416666666666667\n",
      "Classification Report for BoW:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    20030219       0.42      0.24      0.30        42\n",
      "    20030220       0.40      0.62      0.49        48\n",
      "    20030221       0.19      0.13      0.16        30\n",
      "\n",
      "    accuracy                           0.37       120\n",
      "   macro avg       0.34      0.33      0.32       120\n",
      "weighted avg       0.35      0.37      0.34       120\n",
      "\n",
      "\n",
      "Classification Report for TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    20030219       0.25      0.10      0.14        42\n",
      "    20030220       0.36      0.75      0.49        48\n",
      "    20030221       0.20      0.03      0.06        30\n",
      "\n",
      "    accuracy                           0.34       120\n",
      "   macro avg       0.27      0.29      0.23       120\n",
      "weighted avg       0.28      0.34      0.26       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words (BoW) using CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(train_data)\n",
    "X_test_count = count_vectorizer.transform(test_data)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier on BoW\n",
    "model_count = MultinomialNB()\n",
    "model_count.fit(X_train_count, train_labels)\n",
    "predictions_count = model_count.predict(X_test_count)\n",
    "accuracy_count = accuracy_score(test_labels, predictions_count)\n",
    "print(f\"Accuracy with BoW: {accuracy_count}\")\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier on TF-IDF\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, train_labels)\n",
    "predictions_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(test_labels, predictions_tfidf)\n",
    "print(f\"Accuracy with TF-IDF: {accuracy_tfidf}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"Classification Report for BoW:\")\n",
    "print(classification_report(test_labels, predictions_count))\n",
    "\n",
    "print(\"\\nClassification Report for TF-IDF:\")\n",
    "print(classification_report(test_labels, predictions_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5feab69",
   "metadata": {},
   "source": [
    "# How TF-IDF Works:\n",
    "Term Frequency (TF): Measures how often a term (word) appears in a document. It is calculated as the ratio of the number of occurrences of a term to the total number of terms in a document.\n",
    "Inverse Document Frequency (IDF): Measures the importance of a term across a collection of documents. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term.\n",
    "TF-IDF: The product of TF and IDF. It reflects both the local importance (within a document) and the global importance (across documents) of a term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44156b78",
   "metadata": {},
   "source": [
    "# Significance of TF-IDF:\n",
    "Capturing Importance: TF-IDF assigns higher weights to terms that are frequent within a document but relatively rare across the entire document collection.\n",
    "Discriminative Power: Terms that are common in a specific document but uncommon in other documents can have higher TF-IDF scores, making them more discriminative for classification tasks.\n",
    "Normalization: TF-IDF normalizes the influence of document length, preventing longer documents from having inherently larger feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb76ab",
   "metadata": {},
   "source": [
    "# Comparison with BoW:\n",
    "BoW represents documents as vectors of term frequencies, without considering the importance of terms.\n",
    "TF-IDF, on the other hand, considers both the frequency and importance of terms, providing a more nuanced representation of document content.\n",
    "TF-IDF often outperforms BoW in capturing the significance of words for classification tasks, especially when dealing with large and diverse document collections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb3661",
   "metadata": {},
   "source": [
    "# One-Hot Encoding is a technique commonly used to represent categorical variables as binary vectors. Each category is represented by a unique binary value, and the entire set of categories is transformed into a binary matrix. However, using One-Hot Encoding directly for text classification may not be the most suitable approach. Here's why:\n",
    "\n",
    "High Dimensionality:\n",
    "\n",
    "One-Hot Encoding creates a binary vector for each unique category, leading to a high-dimensional feature space.\n",
    "In text classification, especially with a large vocabulary, the number of unique words can be extensive, resulting in an extremely high-dimensional one-hot encoded vector for each document.\n",
    "Sparse Representation:\n",
    "\n",
    "One-Hot Encoding produces a sparse matrix where the majority of entries are zero.\n",
    "For text data, most documents contain only a small subset of the entire vocabulary, leading to sparse representations that may be computationally inefficient and memory-intensive.\n",
    "Loss of Sequence Information:\n",
    "\n",
    "One-Hot Encoding treats each word as an independent entity without considering the order or context of words.\n",
    "Text classification often benefits from capturing the sequential nature of language, and One-Hot Encoding discards this valuable sequential information.\n",
    "Semantic Gap:\n",
    "\n",
    "One-Hot Encoding does not capture the semantic relationships between words. Each word is represented as an independent feature, ignoring potential similarities or connections between words.\n",
    "Not Suitable for Continuous Text:\n",
    "\n",
    "In text classification tasks, where the input is a sequence of words forming coherent text, One-Hot Encoding does not capture the semantic meaning or relationships between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b26344",
   "metadata": {},
   "source": [
    "# Alternative Approaches for Text Classification:\n",
    "\n",
    "Word Embeddings: Word embeddings, such as Word2Vec, GloVe, or FastText, represent words as dense vectors in a continuous vector space. They capture semantic relationships and can be used as features for text classification.\n",
    "\n",
    "TF-IDF or Bag-of-Words (BoW): These approaches, as discussed earlier, are more suitable for text classification tasks. They represent documents based on word frequencies or TF-IDF scores, capturing some level of context.\n",
    "\n",
    "Sequence Models (e.g., RNNs, LSTMs, and Transformers): These models are designed to capture sequential dependencies in text data and have shown excellent performance in various natural language processing tasks, including text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ddefb",
   "metadata": {},
   "source": [
    "# Impact:\n",
    "\n",
    "Bag-of-Words (BoW): Captures word frequency information, ignoring the order of words. May lead to high-dimensional, sparse feature vectors.\n",
    "TF-IDF: Considers both word frequency and importance, providing a more nuanced representation. Often outperforms BoW.\n",
    "One-Hot Encoding: Not directly applicable to text features due to high dimensionality and loss of sequential information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919784da",
   "metadata": {},
   "source": [
    "# Recommendations:\n",
    "TF-IDF for Text Representation:\n",
    "\n",
    "TF-IDF captures both word frequency and importance, making it a suitable choice for text classification tasks. It often outperforms BoW.\n",
    "Word Embeddings (Optional):\n",
    "\n",
    "Consider using pre-trained word embeddings (e.g., Word2Vec, GloVe) for richer semantic representations if the dataset is large enough.\n",
    "N-grams (Optional):\n",
    "\n",
    "Experiment with N-grams, especially bigrams and trigrams, to capture sequential dependencies if the dataset size allows. This can improve the model's understanding of context.\n",
    "Consider Advanced Models:\n",
    "\n",
    "Explore more advanced models like sequence models (RNNs, LSTMs, Transformers) for tasks where capturing sequential dependencies is crucial.\n",
    "In summary, TF-IDF is recommended as the primary feature engineering strategy for text classification, with optional exploration of word embeddings and N-grams based on the specific characteristics and size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ea392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
